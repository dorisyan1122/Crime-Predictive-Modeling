---
title: "Predictive Model"
author: "Ziwen Lu, Xuyan Xiu, Doris Yan"
format: html
editor: visual
execute: 
  warning: false
  cache: true
embed-resources: true
---

```{r load packages, echo=FALSE}
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(parsnip)
library(baguette)
library(doFuture)
library(doParallel)
library(xgboost)
library(vip)
library(haven)
library(here)
library(icpsrdata)
dir.create("data")
```

# Data Wrangling

## Reproducible data retrieval

```{r}
options("icpsr_email" = "dy212@georgetown.edu", "icpsr_password" = "Fbp9nbmKreLsubd8nL5H")
```

### crime

```{r}
# takes 1-2 mins
icpsr_download(
  file_id = 38649,
  download_dir = here("data")
)

crime<-read_dta(here("data","ICPSR_38649","DS0001","38649-0001-Data.dta"))
```

### ses

```{r}
# takes 9-10 mins
icpsr_download(
  file_id = 38528,
  download_dir = here("data")
)

ses08_17<-read_dta(here("data","ICPSR_38528","DS0002","38528-0002-Data.dta"))
```

### school counts

```{r}
# takes 1-2 mins
icpsr_download(
  file_id = 38569,
  download_dir = here("data")
)

schoolct<-read_dta(here("data","ICPSR_38569","DS0001","38569-0001-Data.dta"))
```

### the number of parks

```{r}
# takes 1-2 mins
icpsr_download(
  file_id = 38586,
  download_dir = here("data")
)

parks<-read_dta(here("data","ICPSR_38586","DS0001","38586-0001-Data.dta"))
```

### street connectivity

```{r}
# takes 1-2 mins
icpsr_download(
  file_id = 38580,
  download_dir = here("data")
)

streetcon<-read_dta(here("data","ICPSR_38580","DS0001","38580-0001-Data.dta"))
```

### pollution

```{r}
# takes 1 min
icpsr_download(
  file_id = 38597,
  download_dir = here("data")
)

pollution<-read_dta(here("data","ICPSR_38597","DS0001","38597-0001-Data.dta"))
```

### urbanicity

```{r}
# takes 1 min
icpsr_download(
  file_id = 38606,
  download_dir = here("data")
)

urban<-read_dta(here("data","ICPSR_38606","DS0001","38606-0001-Data.dta"))
```

### land cover

```{r}
# takes 1-2 min
icpsr_download(
  file_id = 38598,
  download_dir = here("data")
)

landcover<-read_dta(here("data","ICPSR_38598","DS0001","38598-0001-Data.dta"))
```

### road system

```{r}
# takes 1-2 min
icpsr_download(
  file_id = 38585,
  download_dir = here("data")
)

road<-read_dta(here("data","ICPSR_38585","DS0001","38585-0001-Data.dta"))
```

## Clean data

### crime

```{r}
# crime data at 2013
crime_data<-crime|>
  rename_all(tolower)|>
  rename(county=stcofips)|>
  select(county,year,viol,property,cpopcrim)

crime_2013<-crime_data|>
  filter(year == 2013)
```

```{r}
clean_data<-
  function(data) {
    cleaned_data <- data |> 
      rename_all(tolower)|>
      mutate(county = substr((tract_fips10), 1, 5))|>
      select(-tract_fips10)

  return(cleaned_data)
}
```

### ses

```{r}
#  ACS five-year estimate 2013-2017
ses_2013<-clean_data(ses08_17)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)|>
  pivot_longer(
  cols = !county, 
  names_to = "indicator",
  values_to = "value"
  )|>
  filter(str_sub(indicator, -2) == "17")|>
  # pivot SES indicators back as variables
  pivot_wider(
    names_from = indicator,
    values_from = value
  )
```

### school counts

```{r}
# school counts at 2013
schoolct_2013<-clean_data(schoolct)|>
  group_by(county,year)|>
  summarise_all(sum, na.rm = TRUE)|>
  filter(year == 2013)
```

### parks

```{r}
# from 2010 decennial census
parks_2013<-clean_data(parks)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)
```

### street connectivity

```{r}
# from 2010 decennial census
streetcon_2013<-clean_data(streetcon)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)
```

### pollution

```{r}
# pollution in 2013
pollution_2013<-clean_data(pollution)|>
  group_by(county,year)|>
  summarise_all(sum, na.rm = TRUE)|>
  filter(year == 2013)
```

### urbanicity

```{r}
# from 2010 decennial census
urban_2013<-clean_data(urban)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)
```

### land cover

```{r}
# from 2010 decennial census
landcover_2013<-landcover|>
  rename_all(tolower)|>
  mutate(county = substr((tract_fips), 1, 5))|>
  select(-tract_fips)|>
  group_by(county, year_intp)|>
  summarise_all(sum, na.rm = TRUE) |>
  filter(year_intp == 2013)
```

### road system

```{r}
# from 2010 decennial census
road_2013<-clean_data(road)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)
```

## Combine All Datasets

```{r}
# combine the data sets with the by "county" and "year" columns
crime_all <- full_join(
  crime_2013,
  full_join(
    ses_2013,
   full_join(
     schoolct_2013,
    full_join(
      parks_2013,
      full_join(
        streetcon_2013,
          full_join(
            pollution_2013, 
            full_join(
              urban_2013,
              full_join(
                landcover_2013, road_2013, by = "county")
              ),
            by = "county"
            ),
          by = "county"
          ),
        by = "county"
        ),
      by = "county"
      ),
    by = "county"
    ),
   by = "county"
  )

crime_all<-crime_all|>
  select(-year.y, -year.x, -year_intp)
```

# EDA and Data cleaning

```{r}
# remove STATA attributes
attr(crime_all$viol, "format.stata") <- NULL
attr(crime_all$property, "format.stata") <- NULL
attr(crime_all$cpopcrim, "format.stata") <- NULL

attr(crime_all$viol, "label") <- NULL
attr(crime_all$property, "label") <- NULL
attr(crime_all$cpopcrim, "label") <- NULL
 
str(crime_all)

# generate log(violent crime) and log (property crime)
crime_all_cleaned <- crime_all|>
  # generate crime rate per 100,000 people and the log value of it
  mutate(log_viol = log((viol / cpopcrim) * 100000),
       log_property = log((property / cpopcrim) * 100000)) |>
  filter(!is.na(log_viol) & !is.na(log_property)) |>
  filter(!(is.infinite(log_viol) | is.infinite(log_property)))|>
  # cpopcrim, tot_pop_census, and totpop13_17 are all total population estimate. cpopcrim is from crime dataset, tot_pop_census is from urbanicity dataset, totpop13_17 is from ses dataset. 
  # since we are predicting crime rate, it would be more appropriate to use the population variable in the crime dataset, thus we will use cpopcrim as population estimate
  select(-tot_pop_census,
         -totpop13_17)

# explore missing values
missing_values <- colSums(is.na(crime_all_cleaned))

missing_values

missing_values<-as.data.frame(missing_values)

# look at the variables with the most missing values, no variable has over 50% missing values
missing_values_percent<-missing_values|>
  mutate(missing_percent = missing_values/nrow(crime_all_cleaned))|>
  arrange(desc(missing_percent))|>
  top_n(10)
```

# Splitting Data

```{r}
set.seed(904)
crime_split <- initial_split(crime_all_cleaned, prop = 0.8)

crime_train <- training(crime_split)
crime_test <- testing(crime_split)
```

# Cross-validation

```{r}
# set up 5-fold cross validation
set.seed(904)
crime_folds <- vfold_cv(data = crime_train, v = 5)
```

# Violent Crime

## Ridge Regression

### feature and target engineering

```{r}
# create a recipe
ridge_v_rec <- 
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())
 
# check if feature engineering works to remove variables
ridge_v_summary<-summary(ridge_v_rec)

# model
ridge_v_mod <- 
  linear_reg(penalty = tune(), mixture = 0) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "glmnet")  

ridge_v_grid<-
  grid_regular(penalty(),
               levels = 20)
# workflow
ridge_v_wf <- 
  workflow() |>
  add_recipe(recipe = ridge_v_rec) |>
  add_model(spec = ridge_v_mod)
```

### choose the best model

```{r}
ridge_v_resamples <-
  ridge_v_wf |>
  tune_grid(
    resamples = crime_folds,
    grid = ridge_v_grid,
    metrics = metric_set(rmse)
    )

ridge_v_resamples |>
  collect_metrics()

top_ridge_v_models <-
  ridge_v_resamples |>
  show_best() |>
  arrange(penalty) 

ridge_v_resamples|>
  select_best()

ridge_v_resamples|>
  autoplot()
```

### Prediction

```{r}
set.seed(904)
ridge_v_pre_wf <- 
  ridge_v_wf |> 
  finalize_workflow(select_best(ridge_v_resamples))

ridge_v_pre_wf<-
  fit(ridge_v_pre_wf, 
      data = crime_train)

ridge_v_predictions <- 
  predict(object = ridge_v_pre_wf, 
          new_data = crime_test)$.pred

county <-
  crime_test$county
```

## Lasso

### feature and target engineering

```{r}
lasso_v_rec <- 
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

# check if feature engineering works to remove variables
lasso_v_summary<-summary(lasso_v_rec)

# create a model
lasso_v_mod <- 
  linear_reg(penalty = tune(), mixture = 1) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "glmnet")  
 
lasso_v_grid <-
  grid_regular(penalty(),
               levels = 20)

lasso_v_wf <-
  workflow() |>
  add_recipe(recipe = lasso_v_rec) |>
  add_model(spec = lasso_v_mod)
```

### choose the best model

```{r}
set.seed(904)
lasso_v_resamples<- 
  lasso_v_wf|>
  tune_grid(
    resamples = crime_folds,
    grid = lasso_v_grid,
    metrics = metric_set(rmse)
    )

lasso_v_resamples |>
  collect_metrics()
  
top_lasso_v_models <-
  lasso_v_resamples |>
  show_best() |>
  arrange(penalty) 

lasso_v_resamples|>
  select_best()

lasso_v_resamples|>
  autoplot()
```

## Random Forest

### feature and target engineering

```{r}
rf_v_rec <- 
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

rf_v_mod <- 
  rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 200
  ) |>
  set_mode(mode = "regression") |>
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

rf_v_wf <- 
  workflow() |>
  add_recipe(rf_v_rec) |>
  add_model(rf_v_mod)

rf_v_grid <- grid_regular(
  mtry(range = c(1, 15)),
  min_n(range = c(1, 15)),
  levels = 5
)
```

### choose the best model

```{r}
set.seed(904)
rf_v_resamples <- 
  tune_grid(
  rf_v_wf,
  resamples = crime_folds,
  grid = rf_v_grid)

collect_metrics(rf_v_resamples)

show_best(rf_v_resamples, 
          metric = "rmse") 

select_best(rf_v_resamples)
autoplot(rf_v_resamples)
```

### Selecting the top 30 important variables

```{r}
rf_v_final_wf <- 
  rf_v_wf |> 
  finalize_workflow(select_best(rf_v_resamples))

rf_v_fit <- 
  rf_v_final_wf |>
  fit(data = crime_all_cleaned)

crime_labels <- c(
  pnhblack13_17 = "Proportion of people non-Hispanic Black",
  pin1b_13_17 = "Proportion of families with Income less than 15K",
  disadvantage13_17 = "Mean of pnhblack pfhfam ppubas ppov punemp",
  ppov13_17 = "Prop. ppl w/ income past 12 months below poverty level",
  ped1_13_17 = "Prop. w/ less than high school diploma",
  pfhfam13_17 = "Proportion female-headed families w/ kids",
  pin2b_13_17 = "Proportion of families with Income 15-30K",
  prop_value_52 = "Prop. Shrub/Scrub (sqm)",
  value_52 = "Shrub/Scrub (sqm)",
  ppubas13_17 = "Prop. of households with public assistance income",
  disadvantage2_13_17 = "Mean of pfhfam ppubas ppov punemp",
  prop_value_41 = "Prop. Deciduous Forest (sqm)",
  value_41 = "Deciduous Forest (sqm)",
  phispanic13_17 = "Proportion of people of Hispanic origin",
  pnwhite13_17 = "Proportion of people non-Hispanic White",
  cpopcrim = "County population", 
  ethnicimmigrant13_17 = "Mean of phispanic pfborn",
  value_71 = "Herbaceuous (sqm)", 
  prop_strlen_secondary = "Prop. secondary road length",
  punemp13_17 = "Prop. 16+ civ labor force unemployed",
  value_90 = "Woody Wetlands (sqm)", 
  tract_area_sqmiles = "Census tract area (square miles)", 
  sum_strlen_secondary = "Total length of secondary roads",
  prop_prim_sec_roads = "Proportion of primary or secondary roads",
  value_81 = "Woody Wetlands (sqm)",
  pfborn13_17 = "Proportion of people who are foreign born",
  pin3b_13_17 = "Proportion of families with Income 30-50K",
  prop_secondary_roads = "Proportion of secondary roads",
  pnhwhite13_17 = "Proportion of people non-Hispanic White",
  value_82 = "Cultivated Crops (sqm)",
  sum_strlen_prim_sec = "Total length of primary or secondary roads",
  pnvmar13_17 = "Proportion of People 15+ Never Married",
  rural_pop = "Total rural population",
  pincgt75k13_17 = "Proportion of families with Income greater than 75K",
  urban_pop = "Total urban population",
  tract_urban_pct = "Proportion of tract population within urban areas",
  prop_value_23 = "Prop. Developed, Medium Intensity (sqm)",
  popden13_17 = "Persons per square mile",
  prop_value_21 = "Prop. Developed, Open Space (sqm)",
  value_24 = "Developed, High Intensity (sqm)",
  prop_value_24 = "Prop. Developed, High Intensity (sqm)",
  pop_density = "Population density (square miles)",
  prop_value_22 = "Prop. Developed, Low Intensity (sqm)",
  prop_value_42 = "Prop. Evergreen Forest (sqm)",
  p18_2913_17 = "Proportion of population 18-29 years of age",
  strntdensity = "Street network density",
  prop_value_90 = "Prop. Woody Wetlands (sqm)",
  value_42 = "Evergreen Forest (sqm)",
  prop_value_43 = "Prop. Mixed Forest (sqm)",
  prop_value_71 = "Prop. Herbaceuous (sqm)",
  pfborn13_17 = "Proportion of people who are foreign born",
  strnetdensity = "Street network density",
  blockdensity = "Block density",
  tract_area_sqmiles.y = "Census tract area (sqm)"
)

rf_v_fit|>
  extract_fit_parsnip()|>
  vip(num_features = 30) %>%
  .$data |>
  mutate(
    Importance = Importance / max(Importance),
    Variable = fct_reorder(.x = Importance, .f = Variable)
  ) |>
  ggplot(aes(Importance, Variable)) +
  scale_y_discrete(labels = crime_labels) +
  labs(title = "Top 30 Variables for Violent Crime Prediction") +
  geom_col()
```

## Linear Regression

### feature and target engineering

```{r}
# build a linear regression recipe
lm_v_rec <-
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

lm_v_mod <- linear_reg() |>
  set_mode(mode = "regression") |>
  set_engine(engine = "lm")

lm_v_wf <- workflow() |>
  add_recipe(lm_v_rec) |>
  add_model(lm_v_mod)
```

### choose the best model

```{r}
set.seed(904)
lm_v_resamples <- lm_v_wf |>
  fit_resamples(resamples = crime_folds)

lm_v_resamples |>
  collect_metrics()
```

## MARS

### feature and target engineering

```{r}
# recipe
mars_v_rec <- 
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

mars_v_rec_sum<-summary(mars_v_rec)

# model
mars_v_mod <- 
  mars(
  num_terms = tune(), 
  prod_degree = tune()
) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "earth")

# tuning grid
mars_v_grid <- 
  grid_regular(
  num_terms(range = c(10, 100)),
  prod_degree(),
  levels = 10
)

# workflow
mars_v_wf <- 
  workflow() |>
  add_recipe(recipe = mars_v_rec) |>
  add_model(spec = mars_v_mod)
```

### choose the best model

```{r}
# resample   
set.seed(904)
mars_v_resamples <- 
  mars_v_wf|>
  tune_grid(
  resamples = crime_folds,
  grid = mars_v_grid,
  metrics = metric_set(rmse)
  )

collect_metrics(mars_v_resamples)

top_mars_v_models <-
  mars_v_resamples |>
  show_best()

mars_v_resamples|>
  select_best()
```

## XG Boost

## feature and target engineering

```{r}
xgb_v_rec <- 
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

all_cores <- parallel::detectCores(logical = FALSE)

registerDoFuture()

cl <- makeCluster(all_cores - 3L)

plan(cluster, workers = cl)

xgb_v_spec <- boost_tree(
  trees = 200,
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     
  sample_size = tune(), 
  mtry = tune(),         
  learn_rate = tune()                   
) |>
  set_engine("xgboost") |>
  set_mode("regression")


set.seed(904)
xgb_v_grid <- 
  grid_latin_hypercube(
  tree_depth(),
  min_n(range = c(2, 10)),
  loss_reduction(range = c(-5, -3)),
  sample_size = sample_prop(),
  mtry(range = c(1, 10)),
  learn_rate(range = c(-5, -1)),
  size = 30
)

xgb_v_wf <- workflow() |>
  add_recipe(xgb_v_rec) |>
  add_model(xgb_v_spec)
```

### choose the best model

```{r}
set.seed(904)
xgb_v_resamples <- 
  tune_grid(
  xgb_v_wf,
  resamples = crime_folds,
  grid = xgb_v_grid)

collect_metrics(xgb_v_resamples)
show_best(xgb_v_resamples, metric = "rmse") 
select_best(xgb_v_resamples, metric= "rmse")
```

## Final comparison of rmse

```{r}
bind_rows(
  `Violent Crime Ridge regression` = collect_metrics(ridge_v_resamples) |>
    filter(.metric == "rmse"),
  `Violent Crime Lasso regression` = collect_metrics(lasso_v_resamples) |>
    filter(.metric == "rmse"),
  `Violent Crime Random Forest` = collect_metrics(rf_v_resamples) |>
    filter(.metric == "rmse"),
  `Violent Crime Linear Regression` = collect_metrics(lm_v_resamples) |>
    filter(.metric == "rmse"),
  `Violent Crime MARS` = collect_metrics(mars_v_resamples) |>
    filter(.metric == "rmse"),  
  `Violent Crime XG Boost`=collect_metrics(xgb_v_resamples) |>
    filter(.metric == "rmse"),
  .id = "model"
) |>
  arrange(mean)
```
Because XG Boost has the lowest RMSE value, we will use MARS as our final model for violent crime.

## Prediction

```{r}

```

# Property Crime

## Ridge Regression

### feature and target engineering

```{r}
# create a recipe
ridge_p_rec <- 
  recipe(log_property ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              property,
              viol,
              log_viol,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())
 
# check if feature engineering works to remove variables
ridge_p_summary<-summary(ridge_p_rec)

# model
ridge_p_mod <- 
  linear_reg(penalty = tune(), mixture = 0) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "glmnet")  

ridge_p_grid<-
  grid_regular(penalty(),
               levels = 20)
# workflow
ridge_p_wf <- 
  workflow() |>
  add_recipe(recipe = ridge_p_rec) |>
  add_model(spec = ridge_p_mod)
```

### choose the best model

```{r}
ridge_p_resamples <-
  ridge_p_wf |>
  tune_grid(
    resamples = crime_folds,
    grid = ridge_p_grid,
    metrics = metric_set(rmse)
    )

ridge_p_resamples |>
  collect_metrics()

top_ridge_p_models <-
  ridge_p_resamples |>
  show_best() |>
  arrange(penalty) 

ridge_p_resamples|>
  select_best()

ridge_p_resamples|>
  autoplot()
```

## Lasso

### feature and target engineering

```{r}
lasso_p_rec <- 
  recipe(log_property ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              property,
              viol,
              log_viol,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

# check if feature engineering works to remove variables
lasso_p_summary<-summary(lasso_p_rec)

# create a model
lasso_p_mod <- 
  linear_reg(penalty = tune(), mixture = 1) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "glmnet")  
 
lasso_p_grid <-
  grid_regular(penalty(),
               levels = 20)

lasso_p_wf <-
  workflow() |>
  add_recipe(recipe = lasso_p_rec) |>
  add_model(spec = lasso_p_mod)
```

### choose the best model

```{r}
set.seed(904)
lasso_p_resamples<- 
  lasso_p_wf|>
  tune_grid(
    resamples = crime_folds,
    grid = lasso_p_grid,
    metrics = metric_set(rmse)
    )

lasso_p_resamples |>
  collect_metrics()
  
top_lasso_p_models <-
  lasso_p_resamples |>
  show_best() |>
  arrange(penalty) 

lasso_p_resamples|>
  select_best()

lasso_p_resamples|>
  autoplot()
```

## Random Forest

### feature and target engineering

```{r}
rf_p_rec <- 
  recipe(log_property ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              property,
              viol,
              log_viol,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

rf_p_mod <- 
  rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 200
  ) |>
  set_mode(mode = "regression") |>
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

rf_p_wf <- 
  workflow() |>
  add_recipe(rf_p_rec) |>
  add_model(rf_p_mod)

rf_p_grid <- grid_regular(
  mtry(range = c(1, 15)),
  min_n(range = c(1, 15)),
  levels = 5
)
```

### choose the best model

```{r}
set.seed(904)
rf_p_resamples <- 
  tune_grid(
  rf_p_wf,
  resamples = crime_folds,
  grid = rf_p_grid)

collect_metrics(rf_p_resamples)

show_best(rf_p_resamples, 
          metric = "rmse") 

select_best(rf_p_resamples)
autoplot(rf_p_resamples)
```

### Selecting the top 30 important variables

```{r}
rf_p_final_wf <- 
  rf_p_wf |> 
  finalize_workflow(select_best(rf_p_resamples))

rf_p_fit <- 
  rf_p_final_wf |>
  fit(data = crime_all_cleaned)

rf_p_fit|>
  extract_fit_parsnip()|>
  vip(num_features = 30) %>%
  .$data |>
  mutate(
    Importance = Importance / max(Importance),
    Variable = fct_reorder(.x = Importance, .f = Variable)
  ) |>
  ggplot(aes(Importance, Variable)) +
  scale_y_discrete(labels = crime_labels) +
  labs(title = "Top 30 Variables for Property Crime Prediction") +
  geom_col()
```

## Linear Regression

### feature and target engineering

```{r}
# build a linear regression recipe
lm_p_rec <-
  recipe(log_property ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              property,
              viol,
              log_viol,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

lm_p_mod <- linear_reg() |>
  set_mode(mode = "regression") |>
  set_engine(engine = "lm")

lm_p_wf <- workflow() |>
  add_recipe(lm_p_rec) |>
  add_model(lm_p_mod)
```

### choose the best model

```{r}
set.seed(904)
lm_p_resamples <- lm_p_wf |>
  fit_resamples(resamples = crime_folds)

lm_p_resamples |>
  collect_metrics()
```

## MARS

### feature and target engineering

```{r}
# recipe
mars_p_rec <- 
  recipe(log_property ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              property,
              viol,
              log_viol,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

mars_p_rec_sum<-summary(mars_p_rec)

# model
mars_p_mod <- 
  mars(
  num_terms = tune(), 
  prod_degree = tune()
) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "earth")

# tuning grid
mars_p_grid <- 
  grid_regular(
  num_terms(range = c(10, 100)),
  prod_degree(),
  levels = 10
)

# workflow
mars_p_wf <- 
  workflow() |>
  add_recipe(recipe = mars_p_rec) |>
  add_model(spec = mars_p_mod)
```

### choose the best model

```{r}
# resample   
set.seed(904)
mars_p_resamples <- 
  mars_p_wf|>
  tune_grid(
  resamples = crime_folds,
  grid = mars_p_grid,
  metrics = metric_set(rmse)
  )

collect_metrics(mars_p_resamples)

top_mars_p_models <-
  mars_p_resamples |>
  show_best()

mars_p_resamples|>
  select_best()
```

## XG Boost

## feature and target engineering

```{r}
xgb_p_rec <- 
  recipe(log_property ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              property,
              viol,
              log_viol,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

all_cores <- parallel::detectCores(logical = FALSE)

registerDoFuture()

cl <- makeCluster(all_cores - 3L)

plan(cluster, workers = cl)

xgb_p_spec <- boost_tree(
  trees = 200,
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     
  sample_size = tune(), 
  mtry = tune(),         
  learn_rate = tune()                   
) |>
  set_engine("xgboost") |>
  set_mode("regression")


set.seed(904)
xgb_p_grid <- 
  grid_latin_hypercube(
  tree_depth(),
  min_n(range = c(2, 10)),
  loss_reduction(range = c(-5, -3)),
  sample_size = sample_prop(),
  mtry(range = c(1, 10)),
  learn_rate(range = c(-5, -1)),
  size = 30
)

xgb_p_wf <- workflow() |>
  add_recipe(xgb_p_rec) |>
  add_model(xgb_p_spec)
```

### choose the best model

```{r}
set.seed(904)
xgb_p_resamples <- 
  tune_grid(
  xgb_p_wf,
  resamples = crime_folds,
  grid = xgb_p_grid)

collect_metrics(xgb_p_resamples)
show_best(xgb_p_resamples, metric = "rmse") 
select_best(xgb_p_resamples, metric= "rmse")
```

## Final comparison of rmse

```{r}
bind_rows(
  `Property Crime Ridge regression` = collect_metrics(ridge_p_resamples) |>
    filter(.metric == "rmse"),
  `Property Crime Lasso regression` = collect_metrics(lasso_p_resamples) |>
    filter(.metric == "rmse"),
  `Property Crime Random Forest` = collect_metrics(rf_p_resamples) |>
    filter(.metric == "rmse"),
  `Property Crime Linear Regression` = collect_metrics(lm_p_resamples) |>
    filter(.metric == "rmse"),
  `Property Crime MARS` = collect_metrics(mars_p_resamples) |>
    filter(.metric == "rmse"),  
  `Property Crime XG Boost`= collect_metrics(xgb_p_resamples) |>
    filter(.metric == "rmse"),
  .id = "model"
) |>
  arrange(mean)
```
Because XG Boost has the lowest RMSE value, we will use MARS as our final model for violent crime.

# Prediction

## Clean data

### crime

```{r}
# crime data at 2013
crime_data<-crime|>
  rename_all(tolower)|>
  rename(county=stcofips)|>
  select(county,year,viol,property,cpopcrim)

crime_2013<-crime_data|>
  filter(year == 2013)
```

```{r}
clean_data<-
  function(data) {
    cleaned_data <- data |> 
      rename_all(tolower)|>
      mutate(county = substr((tract_fips10), 1, 5))|>
      select(-tract_fips10)

  return(cleaned_data)
}
```

### ses

```{r}
#  ACS five-year estimate 2013-2017
ses_2013<-clean_data(ses08_17)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)|>
  pivot_longer(
  cols = !county, 
  names_to = "indicator",
  values_to = "value"
  )|>
  filter(str_sub(indicator, -2) == "17")|>
  # pivot SES indicators back as variables
  pivot_wider(
    names_from = indicator,
    values_from = value
  )
```

### school counts

```{r}
# school counts at 2013
schoolct_2013<-clean_data(schoolct)|>
  group_by(county,year)|>
  summarise_all(sum, na.rm = TRUE)|>
  filter(year == 2013)
```

### parks

```{r}
# from 2010 decennial census
parks_2013<-clean_data(parks)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)
```

### street connectivity

```{r}
# from 2010 decennial census
streetcon_2013<-clean_data(streetcon)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)
```

### pollution

```{r}
# pollution in 2013
pollution_2013<-clean_data(pollution)|>
  group_by(county,year)|>
  summarise_all(sum, na.rm = TRUE)|>
  filter(year == 2013)
```

### urbanicity

```{r}
# from 2010 decennial census
urban_2013<-clean_data(urban)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)
```

### land cover

```{r}
# from 2010 decennial census
landcover_2013<-landcover|>
  rename_all(tolower)|>
  mutate(county = substr((tract_fips), 1, 5))|>
  select(-tract_fips)|>
  group_by(county, year_intp)|>
  summarise_all(sum, na.rm = TRUE) |>
  filter(year_intp == 2013)
```

### road system

```{r}
# from 2010 decennial census
road_2013<-clean_data(road)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)
```

## Combine All Datasets

```{r}
# combine the data sets with the by "county" and "year" columns
crime_all <- full_join(
  crime_2013,
  full_join(
    ses_2013,
   full_join(
     schoolct_2013,
    full_join(
      parks_2013,
      full_join(
        streetcon_2013,
          full_join(
            pollution_2013, 
            full_join(
              urban_2013,
              full_join(
                landcover_2013, road_2013, by = "county")
              ),
            by = "county"
            ),
          by = "county"
          ),
        by = "county"
        ),
      by = "county"
      ),
    by = "county"
    ),
   by = "county"
  )

crime_all<-crime_all|>
  select(-year.y, -year.x, -year_intp)
```

