---
title: "Predictive Model"
author: "Ziwen Lu, Xuyan Xiu, Doris Yan"
format: html
editor: visual
execute: 
  warning: false
  cache: true
embed-resources: true
---

```{r load packages, echo=FALSE}
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(parsnip)
library(baguette)
library(doFuture)
library(doParallel)
library(xgboost)
library(vip)
library(haven)
library(here)
library(icpsrdata)
dir.create("data")
```

# Data Wrangling

## Reproducible data retrieval

```{r}
options("icpsr_email" = "dy212@georgetown.edu", "icpsr_password" = "Fbp9nbmKreLsubd8nL5H")

# crime predictive data takes 1-2 mins
icpsr_download(
  file_id = 38649,
  download_dir = here("data")
)

# crime implementation crime data takes 1-2 mins
icpsr_download(
  file_id = 37059,
  download_dir = here("data")
)

# Ses data takes 9-10 mins
icpsr_download(
  file_id = 38528,
  download_dir = here("data")
)

# school counts takes 1-2 mins
icpsr_download(
  file_id = 38569,
  download_dir = here("data")
)

# street connectivity takes 1-2 mins
icpsr_download(
  file_id = 38580,
  download_dir = here("data")
)

# pollution takes 1 min
icpsr_download(
  file_id = 38597,
  download_dir = here("data")
)

### land cover takes 1-2 min
icpsr_download(
  file_id = 38598,
  download_dir = here("data")
)
```

## Read data

```{r}
# crime modeling data
crime_predictive<-read_dta(here("data","ICPSR_38649","DS0001","38649-0001-Data.dta"))

# crime implementation data
crime_implementation<-read_dta(here("data","ICPSR_37059","DS0004","37059-0004-Data.dta"))

# ses predictive data and implementation data
ses08_17<-read_dta(here("data","ICPSR_38528","DS0002","38528-0002-Data.dta"))

# school counts data and implementation data
schoolct<-read_dta(here("data","ICPSR_38569","DS0001","38569-0001-Data.dta"))

# street connectivity predictive data
streetcon_predictive<-read_dta(here("data","ICPSR_38580","DS0001","38580-0001-Data.dta"))

# street connectivity implementation data
streetcon_implementation<-read_dta(here("data","ICPSR_38580","DS0003","38580-0003-Data.dta"))

# pollution predictive data and implementation data
pollution<-read_dta(here("data","ICPSR_38597","DS0001","38597-0001-Data.dta"))

# land cover predictive data and implementation data
landcover<-read_dta(here("data","ICPSR_38598","DS0001","38598-0001-Data.dta"))
```

## Clean data

### crime predictive

We use annual measure of crime.

```{r}
crime_2010<-crime_predictive|>
  rename_all(tolower)|>
  rename(county=stcofips)|>
  select(county,year,viol,property,cpopcrim)|>
  filter(year == 2010)|>
  # generate crime rate per 100,000 people and the log transform
  mutate(viol_rate = (viol / cpopcrim) * 100000,
         property_rate = (property / cpopcrim) * 100000)|>
  mutate(log_viol = log(viol_rate),
       log_property = log(property_rate))
```

### crime implementation

```{r}
crime_2016<-crime_implementation|>
  rename_all(tolower)|>
  select(fips_st, fips_cty, cpopcrim, viol, property)|>
  # concatenate fips_st and fips_cty to make county fips
  mutate(fips_st = ifelse(fips_st >= 1 & fips_st <= 9, 
                          paste0("0", fips_st), 
                          as.character(fips_st)) 
         )|>
  mutate(fips_cty = str_pad(fips_cty, width = 3, pad = "0")
         )|>
  mutate(county = str_c(fips_st, fips_cty))|>
  select(-fips_cty, -fips_st)|>
  # generate crime rate per 100,000 people and the log transform
  mutate(viol_rate = (viol / cpopcrim) * 100000,
         property_rate = (property / cpopcrim) * 100000)|>
  mutate(log_viol = log(viol_rate),
       log_property = log(property_rate))|>
  select(county, everything())
  
```

```{r}
# create a function for effective data cleaning
clean_data<-
  function(data) {
    cleaned_data <- data |> 
      rename_all(tolower)|>
      mutate(county = substr((tract_fips10), 1, 5))|>
      select(-tract_fips10)

  return(cleaned_data)
}
```

### ses

SES predictive data of 2010 is from ACS five-year estimate from 2008 to 2012. SES implementation data of 2016 is from ACS five-years estimate from 2013 to 2016.

```{r}
# ACS 2008 to 2012 for year 2010
ses_2010<-clean_data(ses08_17)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)|>
  pivot_longer(
  cols = !county, 
  names_to = "indicator",
  values_to = "value"
  )|>
  filter(str_sub(indicator, -2) == "12")|>
  # pivot SES indicators back as variables
  pivot_wider(
    names_from = indicator,
    values_from = value
  )|>
  # cpopcrim and totpop08_12 are all total population estimate. cpopcrim is from crime dataset, totpop08_12 is from ses dataset. 
  # since we are predicting crime rate, it would be more appropriate to use the population variable in the crime dataset, thus we will use cpopcrim as population estimate
  select(-totpop08_12)
   

# ACS 2013 to 2017 for year 2016
ses_2016<-clean_data(ses08_17)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)|>
  pivot_longer(
  cols = !county, 
  names_to = "indicator",
  values_to = "value"
  )|>
  filter(str_sub(indicator, -2) == "17")|>
  # pivot SES indicators back as variables
  pivot_wider(
    names_from = indicator,
    values_from = value
  )|>
  select(-totpop13_17)
```

### school counts

We use annual school counts.

```{r}
# school counts at 2016
schoolct_2010<-clean_data(schoolct)|>
  group_by(county,year)|>
  summarise_all(sum, na.rm = TRUE)|>
  filter(year == 2010)

schoolct_2016<-clean_data(schoolct)|>
  group_by(county,year)|>
  summarise_all(sum, na.rm = TRUE)|>
  filter(year == 2016)
```

### street connectivity

We use street connectivity data at 2010 for predictive data, and street connectivity data at 2020 for implementation data.

```{r}
# from 2010 school count data
streetcon_2010<-clean_data(streetcon_predictive)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)

# use 2020 data to represent 2016 school counts
streetcon_2016<-streetcon_implementation|>
  rename_all(tolower)|>
  mutate(county = substr((tract_fips20), 1, 5))|>
  select(-tract_fips20)|>
  group_by(county)|>
  summarise_all(sum, na.rm = TRUE)
```

### pollution

We use annual measure of pollution.

```{r}
# pollution in 2010
pollution_2010<-clean_data(pollution)|>
  group_by(county,year)|>
  summarise_all(sum, na.rm = TRUE)|>
  filter(year == 2010)

# pollution in 2016
pollution_2016<-clean_data(pollution)|>
  group_by(county,year)|>
  summarise_all(sum, na.rm = TRUE)|>
  filter(year == 2016)
```

### land cover

We use annual interpolation of land cover.

```{r}
# land cover at 2010
landcover_2010<-landcover|>
  rename_all(tolower)|>
  mutate(county = substr((tract_fips), 1, 5))|>
  select(-tract_fips, -intp_flag)|>
  group_by(county, year_intp)|>
  summarise_all(sum, na.rm = TRUE) |>
  filter(year_intp == 2010)

# land cover at 2016
landcover_2016<-landcover|>
  rename_all(tolower)|>
  mutate(county = substr((tract_fips), 1, 5))|>
  select(-tract_fips, -intp_flag)|>
  group_by(county, year_intp)|>
  summarise_all(sum, na.rm = TRUE) |>
  filter(year_intp == 2016)
```

## Combine All Datasets

### predictive data

```{r}
# combine the data sets with the by "county" column
crime_all_2010 <- full_join(
  crime_2010,
  full_join(
    ses_2010,
   full_join(
     schoolct_2010,
    full_join(
      streetcon_2010,
      full_join(
        pollution_2010, landcover_2010, by = "county"
        ),
      by = "county"
      ),
    by = "county"
    ),
   by = "county"
   ),
  by = "county"
  )

crime_all_2010<-crime_all_2010|>
  select(-year.y, -year.x, -year_intp)
```

### implementation data

```{r}
# combine the data sets with the by "county" and "year" columns
crime_all_2016 <- full_join(
  crime_2016,
  full_join(
    ses_2016,
   full_join(
     schoolct_2016,
    full_join(
      streetcon_2016,
      full_join(
        pollution_2016, landcover_2010, by = "county"
        ),
      by = "county"
      ),
    by = "county"
    ),
   by = "county"
   ),
  by = "county"
  )

crime_all_2016<-crime_all_2016|>
  select(-year.y, -year.x, -year_intp)|>
  select(county, everything())|>
  mutate(year = 2016)
```

## 2010 predictive data EDA and data cleaning

```{r}
# remove STATA format and attributes
attr(crime_all_2010$viol, "format.stata") <- NULL
attr(crime_all_2010$property, "format.stata") <- NULL
attr(crime_all_2010$cpopcrim, "format.stata") <- NULL
attr(crime_all_2010$viol_rate, "format.stata") <- NULL
attr(crime_all_2010$property_rate, "format.stata") <- NULL
attr(crime_all_2010$log_viol, "format.stata") <- NULL
attr(crime_all_2010$log_property, "format.stata") <- NULL


attr(crime_all_2010$viol, "label") <- NULL
attr(crime_all_2010$property, "label") <- NULL
attr(crime_all_2010$cpopcrim, "label") <- NULL
attr(crime_all_2010$viol_rate, "label") <- NULL
attr(crime_all_2010$property_rate, "label") <- NULL
attr(crime_all_2010$log_viol, "label") <- NULL
attr(crime_all_2010$log_property, "label") <- NULL
 
str(crime_all_2010)

remove_string_2010<-function(varname){
  sub("08_12","",varname)
}

crime_all_2010_cleaned<-crime_all_2010|>
  # eliminate missing values for outcome variables
  filter(!is.na(log_viol) | !is.na(log_property)) |>
  filter(!(is.infinite(log_viol) | is.infinite(log_property)))|>
  rename_with(remove_string_2010, contains("08_12"))


# explore missing values
missing_values_2010 <- colSums(is.na(crime_all_2010_cleaned))

missing_values_2010<-as.data.frame(missing_values_2010)

# look at the variables with the most missing values, no variable has over 50% missing values
missing_values_percent_2010<-missing_values_2010|>
  mutate(missing_percent = missing_values_2010/nrow(crime_all_2010_cleaned))|>
  arrange(desc(missing_percent))|>
  top_n(10)
```

## 2016 implementation data EDA and data cleaning

```{r}
# remove STATA format and attributes
attr(crime_all_2016$viol, "format.stata") <- NULL
attr(crime_all_2016$property, "format.stata") <- NULL
attr(crime_all_2016$cpopcrim, "format.stata") <- NULL
attr(crime_all_2016$viol_rate, "format.stata") <- NULL
attr(crime_all_2016$property_rate, "format.stata") <- NULL
attr(crime_all_2016$log_viol, "format.stata") <- NULL
attr(crime_all_2016$log_property, "format.stata") <- NULL


attr(crime_all_2016$viol, "label") <- NULL
attr(crime_all_2016$property, "label") <- NULL
attr(crime_all_2016$cpopcrim, "label") <- NULL
attr(crime_all_2016$viol_rate, "label") <- NULL
attr(crime_all_2016$property_rate, "label") <- NULL
attr(crime_all_2016$log_viol, "label") <- NULL
attr(crime_all_2016$log_property, "label") <- NULL
 
str(crime_all_2016)

remove_string_2016<-function(varname){
  sub("13_17","",varname)
}

crime_all_2016_cleaned<-crime_all_2016|>
  # eliminate missing values for outcome variables
  filter(!is.na(log_viol) | !is.na(log_property)) |>
  filter(!(is.infinite(log_viol) | is.infinite(log_property)))|>
  rename_with(remove_string_2016, contains("13_17"))


# explore missing values
missing_values_2016 <- colSums(is.na(crime_all_2016_cleaned))

missing_values_2016<-as.data.frame(missing_values_2016)

# look at the variables with the most missing values, no variable has over 50% missing values
missing_values_percent_2016<-missing_values_2016|>
  mutate(missing_percent = missing_values_2016/nrow(crime_all_2010_cleaned))|>
  arrange(desc(missing_percent))|>
  top_n(10)
```

# Splitting Data

```{r}
set.seed(904)
crime_split <- initial_split(crime_all_2010_cleaned, prop = 0.8)

crime_train <- training(crime_split)
crime_test <- testing(crime_split)
```

# Cross-validation

```{r}
# set up 5-fold cross validation
set.seed(904)
crime_folds <- vfold_cv(data = crime_train, v = 5)
```

# Violent Crime

## Ridge Regression

### feature and target engineering

```{r}
# create a recipe
ridge_v_rec <- 
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              viol_rate,
              property_rate,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())
 
# check if feature engineering works to remove variables
ridge_v_summary<-summary(ridge_v_rec)

# model
ridge_v_mod <- 
  linear_reg(penalty = tune(), mixture = 0) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "glmnet")  

ridge_v_grid<-
  grid_regular(penalty(),
               levels = 20)
# workflow
ridge_v_wf <- 
  workflow() |>
  add_recipe(recipe = ridge_v_rec) |>
  add_model(spec = ridge_v_mod)
```

### choose the best model

```{r}
ridge_v_resamples <-
  ridge_v_wf |>
  tune_grid(
    resamples = crime_folds,
    grid = ridge_v_grid,
    metrics = metric_set(rmse)
    )

ridge_v_resamples |>
  collect_metrics()

top_ridge_v_models <-
  ridge_v_resamples |>
  show_best() |>
  arrange(penalty) 

ridge_v_resamples|>
  select_best()

ridge_v_resamples|>
  autoplot()
```

### prediction

```{r}
set.seed(904)
ridge_v_pre_wf <- 
  ridge_v_wf |> 
  finalize_workflow(select_best(ridge_v_resamples))

ridge_v_pre_wf<-
  fit(ridge_v_pre_wf, 
      data = crime_train)

ridge_v_predictions <- 
  predict(object = ridge_v_pre_wf, 
          new_data = crime_test)$.pred

actual<-crime_test$log_viol

county<-crime_test$county

ridge_v_predictions_df<-
  tibble(county,actual,ridge_v_predictions)

rmse_ridge<-rmse(ridge_v_predictions_df, actual, ridge_v_predictions)
```

## Lasso

### feature and target engineering

```{r}
lasso_v_rec <- 
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              viol_rate,
              property_rate,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())|>
  step_normalize(all_predictors())

# check if feature engineering works to remove variables
lasso_v_summary<-summary(lasso_v_rec)

# create a model
lasso_v_mod <- 
  linear_reg(penalty = tune(), mixture = 1) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "glmnet")  
 
lasso_v_grid <-
  grid_regular(penalty(),
               levels = 20)

lasso_v_wf <-
  workflow() |>
  add_recipe(recipe = lasso_v_rec) |>
  add_model(spec = lasso_v_mod)
```

### choose the best model

```{r}
set.seed(904)
lasso_v_resamples<- 
  lasso_v_wf|>
  tune_grid(
    resamples = crime_folds,
    grid = lasso_v_grid,
    metrics = metric_set(rmse)
    )

lasso_v_resamples |>
  collect_metrics()
  
top_lasso_v_models <-
  lasso_v_resamples |>
  show_best() |>
  arrange(penalty) 

lasso_v_resamples|>
  select_best()

lasso_v_resamples|>
  autoplot()
```

### prediction

```{r}
set.seed(904)
lasso_v_pre_wf <- 
  lasso_v_wf |> 
  finalize_workflow(select_best(lasso_v_resamples))

lasso_v_pre_wf<-
  fit(lasso_v_pre_wf, 
      data = crime_train)

lasso_v_predictions <- 
  predict(object = lasso_v_pre_wf, 
          new_data = crime_test)$.pred

actual<-crime_test$log_viol

county<-crime_test$county

lasso_v_predictions_df<-
  tibble(county,actual,lasso_v_predictions)

rmse_lasso<-rmse(lasso_v_predictions_df, actual, lasso_v_predictions)
```

## Random Forest

### feature and target engineering

```{r}
rf_v_rec <- 
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              viol_rate,
              property_rate,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())

rf_v_mod <- 
  rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 200
  ) |>
  set_mode(mode = "regression") |>
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

rf_v_wf <- 
  workflow() |>
  add_recipe(rf_v_rec) |>
  add_model(rf_v_mod)

rf_v_grid <- grid_regular(
  mtry(range = c(1, 15)),
  min_n(range = c(1, 15)),
  levels = 5
)
```

### choose the best model

```{r}
set.seed(904)
rf_v_resamples <- 
  tune_grid(
  rf_v_wf,
  resamples = crime_folds,
  grid = rf_v_grid)

collect_metrics(rf_v_resamples)

show_best(rf_v_resamples, 
          metric = "rmse") 

select_best(rf_v_resamples)
autoplot(rf_v_resamples)
```

### predictions

```{r}
set.seed(904)
rf_v_pre_wf <- 
  rf_v_wf |> 
  finalize_workflow(select_best(rf_v_resamples))

rf_v_pre_wf<-
  fit(rf_v_pre_wf, 
      data = crime_train)

rf_v_predictions <- 
  predict(object = rf_v_pre_wf, 
          new_data = crime_test)$.pred

actual<-crime_test$log_viol

county<-crime_test$county

rf_v_predictions_df<-
  tibble(county,actual,rf_v_predictions)

rmse_rf<-rmse(rf_v_predictions_df, actual, rf_v_predictions)
```

### Selecting the top 15 important variables

```{r}
rf_v_final_wf <- 
  rf_v_wf |> 
  finalize_workflow(select_best(rf_v_resamples))

rf_v_fit <- 
  rf_v_final_wf |>
  fit(data = crime_all_2010_cleaned)

crime_labels <- c(
  pnhblack08_12 = "Proportion of people non-Hispanic Black",
  pin1b_08_12 = "Proportion of families with Income less than 15K",
  disadvantage08_12  = "Disadvantage1",
  ppubas08_12   = "Prop. of households with public assistance income",
  ppov08_12  = "Prop. ppl w/ income past 12 months below poverty level",
  value_52 = "Road covered by Shrub/Scrub (sqm)",
  pfhfam08_12 = "Proportion female-headed families w/ kids",
  disadvantage2_08_12 = "Disadvantage2",
  prop_value_41 = "Prop. road covered by deciduous Forest (sqm)",
  pnhwhite08_12 = "Proportion of people non-Hispanic White",
  ped1_08_12 = "Prop. w/ less than high school diploma",
  prop_value_52 = "Prop. road covered by Shrub/Scrub (sqm)",
  phispanic08_12 = "Proportion of people of Hispanic origin",
  pin2b_08_12 = "Proportion of families with Income 15-30K",
  pnwhite08_12 = "Proportion of people non-Hispanic White",
  value_41 = "Road covered by deciduous Forest (sqm)",
  punemp08_12 = "Prop. 16+ civ labor force unemployed",
  prop_value_21 = "Prop. Developed, Open Space (sqm)",
  value_24 = "Road Developed, High Intensity (sqm)",
  prop_value_24 = "Prop road Developed, High Intensity (sqm)",
  prop_value_23 = "Prop. Road Developed, Medium Intensity (sqm)",
  prop_value_22 = "Prop. Road Developed, Low Intensity (sqm)",
  p18_2908_12  = "Proportion of population 18-29 years of age",
  popden08_12 = "Persons per square mile"
)

set.seed(904)
rf_v_fit|>
  extract_fit_parsnip()|>
  vip(num_features = 15) %>%
  .$data |>
  mutate(
    Importance = Importance / max(Importance),
    Variable = fct_reorder(.x = Importance, .f = Variable)
  ) |>
  ggplot(aes(Importance, Variable)) +
  scale_y_discrete(labels = crime_labels) +
  labs(title = "Top 15 Variables for Violent Crime Prediction") +
  geom_col()
```

## Linear Regression

### feature and target engineering

```{r}
# build a linear regression recipe
lm_v_rec <-
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              viol_rate,
              property_rate,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())

lm_v_mod <- linear_reg() |>
  set_mode(mode = "regression") |>
  set_engine(engine = "lm")

lm_v_wf <- workflow() |>
  add_recipe(lm_v_rec) |>
  add_model(lm_v_mod)
```

### choose the best model

```{r}
set.seed(904)
lm_v_resamples <- lm_v_wf |>
  fit_resamples(resamples = crime_folds)

lm_v_resamples |>
  collect_metrics()
```

### prediction

```{r}
set.seed(904)
lm_v_pre_wf <- 
  lm_v_wf |> 
  finalize_workflow(select_best(lm_v_resamples))

lm_v_pre_wf<-
  fit(lm_v_pre_wf, 
      data = crime_train)

lm_v_predictions <- 
  predict(object = lm_v_pre_wf, 
          new_data = crime_test)$.pred

actual<-crime_test$log_viol

county<-crime_test$county

lm_v_predictions_df<-
  tibble(county,actual,lm_v_predictions)

rmse_lm<-rmse(lm_v_predictions_df, actual, lm_v_predictions)

```

## MARS

### feature and target engineering

```{r}
# recipe
mars_v_rec <- 
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              viol_rate,
              property_rate,
              property,
              log_property,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())

mars_v_rec_sum<-summary(mars_v_rec)

# model
mars_v_mod <- 
  mars(
  num_terms = tune(), 
  prod_degree = tune()
) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "earth")

# tuning grid
mars_v_grid <- 
  grid_regular(
  num_terms(range = c(10, 100)),
  prod_degree(),
  levels = 10
)

# workflow
mars_v_wf <- 
  workflow() |>
  add_recipe(recipe = mars_v_rec) |>
  add_model(spec = mars_v_mod)
```

### choose the best model

```{r}
# resample   
set.seed(904)
mars_v_resamples <- 
  mars_v_wf|>
  tune_grid(
  resamples = crime_folds,
  grid = mars_v_grid,
  metrics = metric_set(rmse)
  )

collect_metrics(mars_v_resamples)

top_mars_v_models <-
  mars_v_resamples |>
  show_best()

mars_v_resamples|>
  select_best()
```

### prediction

```{r}
set.seed(904)
mars_v_pre_wf <- 
  mars_v_wf |> 
  finalize_workflow(select_best(mars_v_resamples))

mars_v_pre_wf<-
  fit(mars_v_pre_wf, 
      data = crime_train)

mars_v_predictions <- 
  predict(object = mars_v_pre_wf, 
          new_data = crime_test)$.pred

actual<-crime_test$log_viol

county<-crime_test$county

mars_v_predictions_df<-
  tibble(county,actual,mars_v_predictions)

rmse_mars<-rmse(mars_v_predictions_df, actual, mars_v_predictions)

```

## XG Boost

## feature and target engineering

```{r}
xgb_v_rec <- 
  recipe(log_viol ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              viol_rate,
              property_rate,
              property,
              log_property,
              new_role = "id")|>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())

all_cores <- parallel::detectCores(logical = FALSE)

registerDoFuture()

cl <- makeCluster(all_cores - 3L)

plan(cluster, workers = cl)

xgb_v_spec <- boost_tree(
  trees = 200,
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     
  sample_size = tune(), 
  mtry = tune(),         
  learn_rate = tune()                   
) |>
  set_engine("xgboost") |>
  set_mode("regression")


set.seed(904)
xgb_v_grid <- 
  grid_latin_hypercube(
  tree_depth(),
  min_n(range = c(2, 10)),
  loss_reduction(range = c(-5, -3)),
  sample_size = sample_prop(),
  mtry(range = c(1, 10)),
  learn_rate(range = c(-5, -1)),
  size = 30
)

xgb_v_wf <- workflow() |>
  add_recipe(xgb_v_rec) |>
  add_model(xgb_v_spec)
```

### choose the best model

```{r}
set.seed(904)
xgb_v_resamples <- 
  tune_grid(
  xgb_v_wf,
  resamples = crime_folds,
  grid = xgb_v_grid)

collect_metrics(xgb_v_resamples)
show_best(xgb_v_resamples, metric = "rmse") 
select_best(xgb_v_resamples, metric= "rmse")
```

### prediction

```{r}
set.seed(904)
xgb_v_pre_wf <- 
  xgb_v_wf |> 
  finalize_workflow(select_best(xgb_v_resamples))

xgb_v_pre_wf<-
  fit(xgb_v_pre_wf, 
      data = crime_train)

xgb_v_predictions <- 
  predict(object = xgb_v_pre_wf, 
          new_data = crime_test)$.pred

actual<-crime_test$log_viol

county<-crime_test$county

xgb_v_predictions_df<-
  tibble(county,actual,xgb_v_predictions)

rmse_xgb<-rmse(xgb_v_predictions_df, actual, xgb_v_predictions)
```

## Final comparison of rmse

```{r}
# data_frame(Model = c("Violent Crime Ridge regression",
#                  "Violent Crime Lasso regression",
#                  "Violent Crime Random Forest",
#                  "Violent Crime Linear Regression",
#                  "Violent Crime MARS",
#                  "Violent Crime XG Boost"),
#        RMSE = (rmse_lasso, rmse_ridge, rmse_rf, rmse_lm, rmse_mars, rmse_xgb)
# )

bind_rows(`Violent Crime Ridge regression`= rmse_lasso, 
          `Violent Crime Lasso regression`= rmse_ridge, 
          `Violent Crime Random Forest` = rmse_rf, 
          `Violent Crime Linear Regression` = rmse_lm, 
          `Violent Crime MARS` = rmse_mars,
          `Violent Crime XG Boost`= rmse_xgb)


```

Because random forest and XG Boost have the lowest RMSE value, we will use random forest and XG boost as models for violent crime.

# Property Crime

## Random Forest

### feature and target engineering

```{r}
rf_p_rec <- 
  recipe(log_property ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              viol_rate,
              property_rate,
              property,
              log_viol,
              new_role = "id") |>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())

rf_p_mod <- 
  rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 200
  ) |>
  set_mode(mode = "regression") |>
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

rf_p_wf <- 
  workflow() |>
  add_recipe(rf_p_rec) |>
  add_model(rf_p_mod)

rf_p_grid <- grid_regular(
  mtry(range = c(1, 15)),
  min_n(range = c(1, 15)),
  levels = 5
)
```

### choose the best model

```{r}
set.seed(904)
rf_p_resamples <- 
  tune_grid(
  rf_p_wf,
  resamples = crime_folds,
  grid = rf_p_grid)

collect_metrics(rf_p_resamples)

show_best(rf_p_resamples, 
          metric = "rmse") 

select_best(rf_p_resamples)
autoplot(rf_p_resamples)
```

### predictions

```{r}
set.seed(904)
rf_p_pre_wf <- 
  rf_p_wf |> 
  finalize_workflow(select_best(rf_p_resamples))

rf_p_pre_wf<-
  fit(rf_p_pre_wf, 
      data = crime_train)

rf_p_predictions <- 
  predict(object = rf_p_pre_wf, 
          new_data = crime_test)$.pred

actual<-crime_test$log_property

county<-crime_test$county

rf_p_predictions_df<-
  tibble(county,actual,rf_p_predictions)

rmse_rf_property<-rmse(rf_p_predictions_df, actual, rf_p_predictions)
```

### Selecting the top 15 important variables

```{r}
rf_p_final_wf <- 
  rf_p_wf |> 
  finalize_workflow(select_best(rf_p_resamples))

rf_p_fit <- 
  rf_p_final_wf |>
  fit(data = crime_all_2010_cleaned)

set.seed(904)
rf_p_fit|>
  extract_fit_parsnip()|>
  vip(num_features = 15) %>%
  .$data |>
  mutate(
    Importance = Importance / max(Importance),
    Variable = fct_reorder(.x = Importance, .f = Variable)
  ) |>
  ggplot(aes(Importance, Variable)) +
  scale_y_discrete(labels = crime_labels) +
  labs(title = "Top 15 Variables for Property Crime Prediction") +
  geom_col()
```

## XG Boost

## feature and target engineering

```{r}
xgb_p_rec <- 
  recipe(log_property ~ .,
         data = crime_train) |>
  update_role(county,
              year,
              viol,
              viol_rate,
              property_rate,
              property,
              log_viol,
              new_role = "id")|>
  step_impute_median(all_predictors())|>
  step_nzv(all_predictors())

all_cores <- parallel::detectCores(logical = FALSE)

registerDoFuture()

cl <- makeCluster(all_cores - 3L)

plan(cluster, workers = cl)

xgb_p_spec <- boost_tree(
  trees = 200,
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                     
  sample_size = tune(), 
  mtry = tune(),         
  learn_rate = tune()                   
) |>
  set_engine("xgboost") |>
  set_mode("regression")


set.seed(904)
xgb_p_grid <- 
  grid_latin_hypercube(
  tree_depth(),
  min_n(range = c(2, 10)),
  loss_reduction(range = c(-5, -3)),
  sample_size = sample_prop(),
  mtry(range = c(1, 10)),
  learn_rate(range = c(-5, -1)),
  size = 30
)

xgb_p_wf <- workflow() |>
  add_recipe(xgb_p_rec) |>
  add_model(xgb_p_spec)
```

### choose the best model

```{r}
set.seed(904)
xgb_p_resamples <- 
  tune_grid(
  xgb_p_wf,
  resamples = crime_folds,
  grid = xgb_p_grid)

collect_metrics(xgb_p_resamples)
show_best(xgb_p_resamples, metric = "rmse") 
select_best(xgb_p_resamples, metric= "rmse")
```

### prediction

```{r}
set.seed(904)
xgb_p_pre_wf <- 
  xgb_p_wf |> 
  finalize_workflow(select_best(xgb_p_resamples))

xgb_p_pre_wf<-
  fit(xgb_p_pre_wf, 
      data = crime_train)

xgb_p_predictions <- 
  predict(object = xgb_p_pre_wf, 
          new_data = crime_test)$.pred

actual<-crime_test$log_property

county<-crime_test$county

xgb_p_predictions_df<-
  tibble(county,actual,xgb_p_predictions)

rmse_xgb_property<-rmse(xgb_p_predictions_df, actual, xgb_p_predictions)
```

## Final comparison of rmse

```{r}
bind_rows(`Violent Crime Random Forest` = rmse_rf_property, 
          `Violent Crime XG Boost`= rmse_xgb_property)

```

Because XG Boost has the lowest RMSE value, we will use MARS as our final model for violent crime.

# Prediction on implementation data in 2016

## violent crime
```{r}
set.seed(904)
xgb_v_pre_wf_2016 <- 
  xgb_v_wf |> 
  finalize_workflow(select_best(xgb_v_resamples))

xgb_v_pre_wf_2016<-
  fit(xgb_v_pre_wf, 
      data = crime_all_2010_cleaned)

xgb_v_predictions_2016 <- 
  predict(object = xgb_v_pre_wf_2016, 
          new_data = crime_all_2016_cleaned)$.pred

actual<-crime_all_2016_cleaned$log_viol

county<-crime_all_2016_cleaned$county

xgb_v_predictions_2016_df<-
  tibble(county,actual,xgb_v_predictions_2016)

rmse_xgb_v_2016<-rmse(xgb_v_predictions_2016_df, actual, xgb_v_predictions_2016)
```
## property crime
```{r}
set.seed(904)
rf_p_pre_wf_2016 <- 
  rf_p_wf |> 
  finalize_workflow(select_best(rf_p_resamples))

rf_p_pre_wf_2016<-
  fit(rf_p_pre_wf, 
      data = crime_all_2010_cleaned)

rf_p_predictions_2016 <- 
  predict(object = rf_p_pre_wf_2016, 
          new_data = crime_all_2016_cleaned)$.pred

actual<-crime_all_2016_cleaned$log_property

county<-crime_all_2016_cleaned$county

rf_p_predictions_2016_df<-
  tibble(county,actual,rf_p_predictions_2016)

rmse_rf_p_2016<-rmse(rf_p_predictions_2016_df, actual, rf_p_predictions_2016)
```